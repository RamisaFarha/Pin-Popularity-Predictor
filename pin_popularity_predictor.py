# -*- coding: utf-8 -*-
"""Pin Popularity Predictor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/pin-popularity-predictor-371fa3a8-e3df-495d-81e8-970c2e3541be.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241023/auto/storage/goog4_request%26X-Goog-Date%3D20241023T214422Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db9c26362837eec08cace4ec99b9c2ce2e87e4cf5a2b5659f585571b9adb351c13ff82a71f4d37ad783dba2492d7fc2fcf369e485d5a992f07df3cfb65b334d02d17114e2055406c2cff9ac50f1175ffa16bd6f78bbd930ee39ec3a95a28c87cf0c13d763da1a32c161bfe9f77b49157fb17f461bc6be15ab3f161c8723252b2b0ac968e1a77a6b1282a591274e3befe81628ab23f32ed71bcd61b26f85be18ff51390721ba65e64400bd5686e8e97a7ad126f7ec148f702857377bcaf3ba8ca1f64fd3aaf56ff8c35dde72c841cc4d4e3554fd0bbfb863b9dcf5ba27bafa31edbee40c491d0d203f76d5276386f27d2441da650e4a91b30db96c3552eadebb50
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
oneliwickramasinghe_pinterest_snapshot_of_popularity_and_engagement_path = kagglehub.dataset_download('oneliwickramasinghe/pinterest-snapshot-of-popularity-and-engagement')

print('Data source import complete.')

import pandas as pd

df = pd.read_csv('/kaggle/input/pinterest-snapshot-of-popularity-and-engagement/pinterest_finalised.csv')
df.head()

"""**Data Exploration and Cleaning**"""

df.info()
df.describe()
df.isnull().sum()

"""**Handling Missing Data**"""

# Fill missing text data in 'description' and 'title' with empty strings
df['description'].fillna('', inplace=True)
df['title'].fillna('', inplace=True)

df.isnull().sum()

"""**Label Creation (Defining Viral and Non-Viral Pins)**
* Create a 'Popular' Label
"""

# Define the top 30% and bottom 30% thresholds for the 'repin_count' column
top_threshold = df['repin_count'].quantile(0.70)
bottom_threshold = df['repin_count'].quantile(0.30)

# Create a new column 'popular' to label pins as 1 (popular) or 0 (non-popular)
df['popular'] = df['repin_count'].apply(lambda x: 1 if x >= top_threshold else 0)

df['popular'].value_counts()

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the distribution of repin_count
plt.figure(figsize=(10, 6))
sns.histplot(df['repin_count'], bins=30, kde=True)
plt.title('Distribution of Repin Count')
plt.xlabel('Repin Count')
plt.ylabel('Frequency')
plt.show()

# Visualize the popularity counts
plt.figure(figsize=(8, 5))
sns.countplot(x='popular', data=df)
plt.title('Count of Popular vs Non-Popular Pins')
plt.xlabel('Popular (1) or Non-Popular (0)')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Non-Popular', 'Popular'])
plt.show()

"""**Text Preprocessing**"""

# Create a new feature by combining 'Description' and 'Title'
df['text'] = df['description'] + ' ' + df['title']
# Text Vectorization Using TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

# Set up the TF-IDF vectorizer
tfidf = TfidfVectorizer(max_features=1000)

# Fit the TF-IDF vectorizer on the 'text' feature
X_text = tfidf.fit_transform(df['text']).toarray()

# Resulting matrix
print(X_text.shape)

"""**Train-Test Split**"""

from sklearn.model_selection import train_test_split

# Features (X) and target (y)
X = X_text
y = df['popular']

# Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set size:", X_train.shape)
print("Test set size:", X_test.shape)

"""**Model Building**"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

"""**Logistic Regression Model**"""

# Training Logistic Regression classifier
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

# Predict the labels for the test set
y_pred_logistic = logistic_model.predict(X_test)

# Evaluate the Logistic Regression model
print("Logistic Regression Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_logistic):.2f}")
print(classification_report(y_test, y_pred_logistic))

# Confusion matrix for Logistic Regression
cm_logistic = confusion_matrix(y_test, y_pred_logistic)
disp_logistic = ConfusionMatrixDisplay(confusion_matrix=cm_logistic)
disp_logistic.plot()
plt.title('Logistic Regression Confusion Matrix')
plt.show()

"""**Random Forest Classifier**"""

# Training Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict the labels for the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the Random Forest model
print("\nRandom Forest Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}")
print(classification_report(y_test, y_pred_rf))

# Confusion matrix for Random Forest
cm_rf = confusion_matrix(y_test, y_pred_rf)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf)
disp_rf.plot()
plt.title('Random Forest Confusion Matrix')
plt.show()  # Show the plot

"""**Feature Importance for Random Forest**"""

# Get feature importances from the Random Forest model
importances = rf_model.feature_importances_

# Get the names of the words (features)
feature_names = tfidf.get_feature_names_out()

feature_importance_df = pd.DataFrame({'word': feature_names, 'importance': importances})

feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)

print(feature_importance_df.head(10))

feature_importance_df.head(10).plot(kind='barh', x='word', y='importance', title='Top Words Predicting Viral Pins')
plt.show()

"""**Support Vector Machines (SVM)**"""

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Preprocess the data
df['text'] = df['description'].fillna('') + ' ' + df['title'].fillna('')

# Use repin_count as the feature and create a target variable
X = df[['text', 'repin_count']]
y = df['repin_count'].apply(lambda x: 1 if x > 0 else 0)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Text Vectorization using TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features based on your dataset size
X_train_text = vectorizer.fit_transform(X_train['text'])
X_test_text = vectorizer.transform(X_test['text'])

# Combine vectorized text and repin_count into a single feature set
import scipy
X_train_combined = scipy.sparse.hstack([X_train_text, X_train[['repin_count']].values])
X_test_combined = scipy.sparse.hstack([X_test_text, X_test[['repin_count']].values])

# Apply SMOTE for resampling
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train_combined, y_train)

# Train
svm_model = SVC(class_weight='balanced', random_state=42)
svm_model.fit(X_resampled, y_resampled)

# Evaluate
y_pred_svm = svm_model.predict(X_test_combined)

print("SVM Results:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.2f}")
print(classification_report(y_test, y_pred_svm))

# Confusion Matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title('SVM Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""Accuracy:

* Random Forest: 0.65
* Logistic Regression: 0.65
* SVM: 0.99

The SVM model significantly outperformed both Random Forest and Logistic Regression in terms of accuracy.

* Best Model: The SVM model is clearly the best performer based on all metrics (accuracy, precision, recall, and F1-score).
* Issues with Other Models: Both Random Forest and Logistic Regression struggled particularly with the positive class (Class 1), indicating they might not be well-suited for this specific problem, possibly due to class imbalance.

**Hyperparameter Tuning for SVM**
"""

# Create a binary target variable based on repin counts
threshold = 50  # Adjust this threshold as needed
df['target'] = (df['repin_count'] > threshold).astype(int)

# Check for NaN values in description and title
print("Checking for NaN values in description and title columns:")
print(df[['description', 'title']].isnull().sum())

# Fill NaN values with empty strings or drop rows with NaN values
df['description'].fillna('', inplace=True)
df['title'].fillna('', inplace=True)

# Split the dataset into features and target
X = df[['description', 'title']]
y = df['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize text features
tfidf_vectorizer = TfidfVectorizer()
X_train_vectorized = tfidf_vectorizer.fit_transform(X_train['description'] + ' ' + X_train['title'])
X_test_vectorized = tfidf_vectorizer.transform(X_test['description'] + ' ' + X_test['title'])

# Define the SVM model and hyperparameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# Perform Grid Search for hyperparameter tuning
svm_grid = GridSearchCV(SVC(), param_grid, cv=3, scoring='accuracy')
svm_grid.fit(X_train_vectorized, y_train)

# Output the best parameters and score
print("Best parameters found: ", svm_grid.best_params_)
print("Best cross-validated accuracy: ", svm_grid.best_score_)

# Evaluate the best model on the test set
best_svm_model = svm_grid.best_estimator_
y_pred = best_svm_model.predict(X_test_vectorized)

# Print evaluation metrics
print("SVM Test Accuracy: ", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Example DataFrame setup (you should replace this with your actual dataset loading code)
# df = pd.read_csv('your_dataset.csv')

# Create a binary target variable based on repin counts
threshold = 50  # Adjust this threshold as needed
df['target'] = (df['repin_count'] > threshold).astype(int)

# Check for NaN values in description and title
print("Checking for NaN values in description and title columns:")
print(df[['description', 'title']].isnull().sum())

# Fill NaN values with empty strings
df.fillna({'description': '', 'title': ''}, inplace=True)

# Split the dataset into features and target
X = df[['description', 'title']]
y = df['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize text features
tfidf_vectorizer = TfidfVectorizer()
X_train_vectorized = tfidf_vectorizer.fit_transform(X_train['description'] + ' ' + X_train['title'])
X_test_vectorized = tfidf_vectorizer.transform(X_test['description'] + ' ' + X_test['title'])

# Define the SVM model and hyperparameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# Perform Grid Search for hyperparameter tuning
svm_grid = GridSearchCV(SVC(), param_grid, cv=3, scoring='accuracy')
svm_grid.fit(X_train_vectorized, y_train)

# Output the best parameters and score
print("Best parameters found: ", svm_grid.best_params_)
print("Best cross-validated accuracy: ", svm_grid.best_score_)

# Evaluate the best model on the test set
best_svm_model = svm_grid.best_estimator_
y_pred = best_svm_model.predict(X_test_vectorized)

# Print evaluation metrics
print("SVM Test Accuracy: ", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE

# Create a binary target variable based on repin counts
threshold = 50
df['target'] = (df['repin_count'] > threshold).astype(int)

# Check for NaN values in description and title
print("Checking for NaN values in description and title columns:")
print(df[['description', 'title']].isnull().sum())

# Fill NaN values with empty strings
df['description'] = df['description'].fillna('')
df['title'] = df['title'].fillna('')

# Split the dataset into features and target
X = df[['description', 'title']]
y = df['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize text features
tfidf_vectorizer = TfidfVectorizer()
X_train_vectorized = tfidf_vectorizer.fit_transform(X_train['description'] + ' ' + X_train['title'])
X_test_vectorized = tfidf_vectorizer.transform(X_test['description'] + ' ' + X_test['title'])

# Optionally apply SMOTE to address class imbalance
# Uncomment the following lines if you want to use SMOTE
# smote = SMOTE()
# X_train_vectorized, y_train = smote.fit_resample(X_train_vectorized, y_train)

# Define the SVM model and hyperparameter grid
param_grid = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto']
}

# Perform Grid Search for hyperparameter tuning with class weights
svm_grid = GridSearchCV(SVC(class_weight='balanced'), param_grid, cv=3, scoring='accuracy')
svm_grid.fit(X_train_vectorized, y_train)

# Output the best parameters and score
print("Best parameters found: ", svm_grid.best_params_)
print("Best cross-validated accuracy: ", svm_grid.best_score_)

# Evaluate the best model on the test set
best_svm_model = svm_grid.best_estimator_
y_pred = best_svm_model.predict(X_test_vectorized)

# Print evaluation metrics
print("SVM Test Accuracy: ", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Generate confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Repinned', 'Repinned'], yticklabels=['Not Repinned', 'Repinned'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""**Cross-Validation**"""

from sklearn.model_selection import cross_val_score

# Perform cross-validation
cv_scores = cross_val_score(SVC(C=1, kernel='poly', gamma='scale'), X_train_vectorized, y_train, cv=5)
print("Cross-validated accuracy: ", cv_scores.mean())

"""**Model Saved**"""

import joblib

# Save the trained model
joblib.dump(best_svm_model, 'svm_model.pkl')

# Save the TF-IDF vectorizer
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')